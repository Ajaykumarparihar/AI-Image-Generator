# -*- coding: utf-8 -*-
"""AI Image Generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nd74n5ub-hX6cZ98hoYCJHpdzQ-8220o
"""

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install diffusers transformers accelerate streamlit Pillow

import torch
from diffusers import StableDiffusionPipeline

# GPU ya CPU detect karein
device = "cuda" if torch.cuda.is_available() else "cpu"

# Hugging Face se model download karein
model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
)

# Model ko GPU par move karein (agar available ho)
pipe.to(device)

print("Model Loaded Successfully!")

import time
from PIL import Image

def generate_image(prompt, num_steps=30):
    start_time = time.time()

    with torch.no_grad():  # Faster inference
        image = pipe(
            prompt,
            num_inference_steps=num_steps,
            guidance_scale=7.5
        ).images[0]

    generation_time = time.time() - start_time
    print(f"Image Generated in {generation_time:.2f} seconds!")
    return image

# üî• Example Prompt
prompt = "A futuristic city at sunset"
image = generate_image(prompt, num_steps=20)

# Display Image
image.show()

image_path = "generated_image.png"
image.save(image_path)
print(f"Image saved at {image_path}")

from google.colab import files
files.download(image_path)

!pip install streamlit
!npm install -g localtunnel

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from diffusers import StableDiffusionPipeline
# import torch
# 
# st.title("üñºÔ∏è AI Image Generator")
# 
# # Load Stable Diffusion Model
# @st.cache_resource
# def load_model():
#     model_id = "runwayml/stable-diffusion-v1-5"
#     device = "cuda" if torch.cuda.is_available() else "cpu"
#     pipe = StableDiffusionPipeline.from_pretrained(
#         model_id, torch_dtype=torch.float16 if device == "cuda" else torch.float32
#     )
#     pipe.to(device)
#     return pipe, device
# 
# pipe, device = load_model()
# 
# # User Input
# prompt = st.text_input("Enter a text prompt:", "A futuristic city at sunset")
# num_steps = st.slider("Number of inference steps", 10, 50, 30)
# generate_button = st.button("Generate Image")
# 
# if generate_button:
#     with st.spinner("Generating image..."):
#         image = pipe(prompt, num_inference_steps=num_steps).images[0]
#         st.image(image, caption="Generated Image", use_column_width=True)
#

!pip install streamlit
!pip install pyngrok

from pyngrok import ngrok

# Streamlit app run karna
!streamlit run app.py &

# Ngrok tunnel create karna
public_url = ngrok.connect(port='8501')
print(f"Your Streamlit app is live at: {public_url}")

